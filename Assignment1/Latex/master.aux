\relax 
\providecommand\zref@newlabel[2]{}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Fitting to Air Pollution Data}{1}{section.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Matlab Code}{1}{subsection.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Testing the Software}{1}{subsection.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces LSQ Model fit for n = 3 (blue line) and n = 5 (black line). The figure shows how the model with 3 features fails to capture the bimodal structure of the observed data. The model with 5 features better captures the behavior, but still fails to provide good estimates of the observed data.\relax }}{2}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:figure1}{{1}{2}{LSQ Model fit for n = 3 (blue line) and n = 5 (black line). The figure shows how the model with 3 features fails to capture the bimodal structure of the observed data. The model with 5 features better captures the behavior, but still fails to provide good estimates of the observed data.\relax }{figure.caption.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}The Optimal Order of the Fit}{2}{subsection.1.3}}
\newlabel{eq:firstEquation}{{1}{2}{The Optimal Order of the Fit}{equation.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The figures above show the residuals over time for each model of n'th order. The y-axis describes the magnitude of the residuals while the x-axis represents time in hours. Only the models of order 7, 9 and 11 (with filled blue bubbles) meet the criteria for randomness of signs and absence of trends.\relax }}{3}{figure.caption.2}}
\newlabel{fig:figure2}{{2}{3}{The figures above show the residuals over time for each model of n'th order. The y-axis describes the magnitude of the residuals while the x-axis represents time in hours. Only the models of order 7, 9 and 11 (with filled blue bubbles) meet the criteria for randomness of signs and absence of trends.\relax }{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces LSQ model fit of n order 7, 9 and 11. The figure shows how these models capture the main behavior of the observed data without following it too closely. This leaves room for data error as opposed to adapting to the data error, which is undesirable.\relax }}{4}{figure.caption.3}}
\newlabel{fig:figure3}{{3}{4}{LSQ model fit of n order 7, 9 and 11. The figure shows how these models capture the main behavior of the observed data without following it too closely. This leaves room for data error as opposed to adapting to the data error, which is undesirable.\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces LSQ model fit of n orders 13 to 23. The figure shows how these models follow the observed data too closely and risk adapting the behavior of the data error as well. This can result in overfitting.\relax }}{5}{figure.caption.4}}
\newlabel{fig:figure4}{{4}{5}{LSQ model fit of n orders 13 to 23. The figure shows how these models follow the observed data too closely and risk adapting the behavior of the data error as well. This can result in overfitting.\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The residual norm and its scaled version here are shown as a function of n. The figure shows how initially (for n = 1,3 and 5) the residuals (and the aproximation error) decrease rapidly as each additional n is able to fit a better model. After n = 7, the residuals flatten and the data error dominates the residuals. The transition between these two stages (from n =7 to n= 11) is an optimal zone for the order of the model.\relax }}{5}{figure.caption.5}}
\newlabel{fig:figure5}{{5}{5}{The residual norm and its scaled version here are shown as a function of n. The figure shows how initially (for n = 1,3 and 5) the residuals (and the aproximation error) decrease rapidly as each additional n is able to fit a better model. After n = 7, the residuals flatten and the data error dominates the residuals. The transition between these two stages (from n =7 to n= 11) is an optimal zone for the order of the model.\relax }{figure.caption.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces \relax }}{6}{table.caption.6}}
\newlabel{tab:Table1}{{1}{6}{\relax }{table.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Estimating the Standard Deviation of the Solution Coefficients}{6}{subsection.1.4}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Table 2\relax }}{6}{table.caption.7}}
\newlabel{tab:Table2}{{2}{6}{Table 2\relax }{table.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}Concluding remarks}{7}{subsection.1.5}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Algorithms for Nonlinear Unconstrained Optimization}{8}{section.2}}
\newlabel{eq:himm}{{3}{8}{Algorithms for Nonlinear Unconstrained Optimization}{equation.2.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces \relax }}{8}{table.caption.9}}
\newlabel{tab:statPoints}{{3}{8}{\relax }{table.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Contour plot of the Himmelblau function. The blue dots are local minima, the black dots are saddle points and the red dot is a local maximum.\relax }}{9}{figure.caption.8}}
\newlabel{fig:himmelblau}{{6}{9}{Contour plot of the Himmelblau function. The blue dots are local minima, the black dots are saddle points and the red dot is a local maximum.\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Algorithms used}{9}{subsection.2.1}}
\@writefile{toc}{\contentsline {paragraph}{Backtracking:}{9}{section*.10}}
\newlabel{eq:sufficientDecrease}{{4}{9}{Backtracking:}{equation.2.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Soft line search:}{10}{section*.11}}
\newlabel{eq:curvature}{{5}{10}{Soft line search:}{equation.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Steepest descent}{10}{subsection.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces The steepest descent algorithm on the Himmelblau function from the starting point (1,1), (1,-1) and (-1,0).\relax }}{11}{figure.caption.12}}
\newlabel{fig:steepest}{{7}{11}{The steepest descent algorithm on the Himmelblau function from the starting point (1,1), (1,-1) and (-1,0).\relax }{figure.caption.12}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Summary of the steepest descent algorithm on the Himmelblau function\relax }}{11}{table.caption.14}}
\newlabel{tab:steepest}{{4}{11}{Summary of the steepest descent algorithm on the Himmelblau function\relax }{table.caption.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Newton's algorithm}{11}{subsection.2.3}}
\newlabel{eq:taylor}{{6}{11}{Newton's algorithm}{equation.2.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Error plot for the steepest descent algorithm on the Himmelblau function. Plots are from starting points (1,1), (1,-1) and (-1,0) respectively.\relax }}{12}{figure.caption.13}}
\newlabel{fig:rateConvSteepest}{{8}{12}{Error plot for the steepest descent algorithm on the Himmelblau function. Plots are from starting points (1,1), (1,-1) and (-1,0) respectively.\relax }{figure.caption.13}{}}
\newlabel{eq:newtondir}{{7}{12}{Newton's algorithm}{equation.2.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Newton's algorithm on the Himmelblau function from the starting point (-2,0), (1,-1) and (-4,-2).\relax }}{13}{figure.caption.15}}
\newlabel{fig:newton}{{9}{13}{Newton's algorithm on the Himmelblau function from the starting point (-2,0), (1,-1) and (-4,-2).\relax }{figure.caption.15}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Summary of Newton's algorithm on the Himmelblau function\relax }}{13}{table.caption.17}}
\newlabel{tab:newton}{{5}{13}{Summary of Newton's algorithm on the Himmelblau function\relax }{table.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Error plot for Newton's algorithm on the Himmelblau function. Plots are from starting points (-2,0), (1,-1) and (-4,-2) respectively.\relax }}{14}{figure.caption.16}}
\newlabel{fig:rateConvNewton}{{10}{14}{Error plot for Newton's algorithm on the Himmelblau function. Plots are from starting points (-2,0), (1,-1) and (-4,-2) respectively.\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Quasi-Newton algorithm}{14}{subsection.2.4}}
\newlabel{eq:secantCon1}{{8}{14}{Quasi-Newton algorithm}{equation.2.8}{}}
\newlabel{eq:secantCon2}{{9}{15}{Quasi-Newton algorithm}{equation.2.9}{}}
\newlabel{eq:secantCon3}{{11}{15}{Quasi-Newton algorithm}{equation.2.11}{}}
\newlabel{eq:BFGS}{{12}{15}{Quasi-Newton algorithm}{equation.2.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Quasi Newton algorithm on the Himmelblau function from the starting point (1,1), (1,-1) and (-1,0).\relax }}{15}{figure.caption.18}}
\newlabel{fig:quasiNewton}{{11}{15}{Quasi Newton algorithm on the Himmelblau function from the starting point (1,1), (1,-1) and (-1,0).\relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Error plot for Quasi Newton algorithm on the Himmelblau function. Plots are from starting points (1,1), (1,-1) and (-1,0) respectively.\relax }}{16}{figure.caption.19}}
\newlabel{fig:rateConvQuasiNewton}{{12}{16}{Error plot for Quasi Newton algorithm on the Himmelblau function. Plots are from starting points (1,1), (1,-1) and (-1,0) respectively.\relax }{figure.caption.19}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Summary of the Quasi Newton algorithm on the Himmelblau function\relax }}{16}{table.caption.20}}
\newlabel{tab:quasiNewton}{{6}{16}{Summary of the Quasi Newton algorithm on the Himmelblau function\relax }{table.caption.20}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Gauss-Newton algorithm}{16}{subsection.2.5}}
\newlabel{eq:leastSq}{{13}{16}{Gauss-Newton algorithm}{equation.2.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Gauss-Newton algorithm on the Himmelblau function from the starting point (1,1), (1,-1) and (-1,0).\relax }}{17}{figure.caption.21}}
\newlabel{fig:gaussNewton}{{13}{17}{Gauss-Newton algorithm on the Himmelblau function from the starting point (1,1), (1,-1) and (-1,0).\relax }{figure.caption.21}{}}
\newlabel{eq:gaussNewton1}{{14}{17}{Gauss-Newton algorithm}{equation.2.14}{}}
\newlabel{eq:gaussNewton2}{{15}{17}{Gauss-Newton algorithm}{equation.2.15}{}}
\newlabel{eq:gaussNewton3}{{16}{17}{Gauss-Newton algorithm}{equation.2.16}{}}
\newlabel{eq:gaussNewton4}{{17}{17}{Gauss-Newton algorithm}{equation.2.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Error plot for Gauss-Newton algorithm on the Himmelblau function. Plots are from starting points (1,1), (1,-1) and (-1,0) respectively.\relax }}{18}{figure.caption.22}}
\newlabel{fig:rateConvGaussNewton}{{14}{18}{Error plot for Gauss-Newton algorithm on the Himmelblau function. Plots are from starting points (1,1), (1,-1) and (-1,0) respectively.\relax }{figure.caption.22}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Summary of the Gauss-Newton algorithm on the Himmelblau function\relax }}{18}{table.caption.23}}
\newlabel{tab:gaussNewton}{{7}{18}{Summary of the Gauss-Newton algorithm on the Himmelblau function\relax }{table.caption.23}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Levenberg-Marquardt}{19}{subsection.2.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Levenberg-Marquardt algorithm on the Himmelblau function from the starting point (1,1), (1,-1) and (-1,0).\relax }}{19}{figure.caption.24}}
\newlabel{fig:LM}{{15}{19}{Levenberg-Marquardt algorithm on the Himmelblau function from the starting point (1,1), (1,-1) and (-1,0).\relax }{figure.caption.24}{}}
\newlabel{eq:dampedNewton}{{19}{19}{Levenberg-Marquardt}{equation.2.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Error plot for Levenberg-Marquardt algorithm on the Himmelblau function. Plots are from starting points (1,1), (1,-1) and (-1,0) respectively.\relax }}{20}{figure.caption.25}}
\newlabel{fig:rateConvLM}{{16}{20}{Error plot for Levenberg-Marquardt algorithm on the Himmelblau function. Plots are from starting points (1,1), (1,-1) and (-1,0) respectively.\relax }{figure.caption.25}{}}
\newlabel{eq:LM}{{20}{20}{Levenberg-Marquardt}{equation.2.20}{}}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces Summary of the Levenberg-Marquardt algorithm on the Himmelblau function\relax }}{20}{table.caption.26}}
\newlabel{tab:LM}{{8}{20}{Summary of the Levenberg-Marquardt algorithm on the Himmelblau function\relax }{table.caption.26}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7}Comparison of algorithms}{21}{subsection.2.7}}
\@writefile{lot}{\contentsline {table}{\numberline {9}{\ignorespaces Summary of the methods discussed. For the line search methods -- Steepest descent, Quasi Newton, Gauss-Newton and Newton -- only the results from the soft line search are included. The results from fminunc (using Levenberg-Marquardt) from matlab are also included. Notice that all algorithms have the same starting points except Newton's algorithm, since it failed to converge for all of them. \relax }}{21}{table.caption.27}}
\newlabel{tab:all}{{9}{21}{Summary of the methods discussed. For the line search methods -- Steepest descent, Quasi Newton, Gauss-Newton and Newton -- only the results from the soft line search are included. The results from fminunc (using Levenberg-Marquardt) from matlab are also included. Notice that all algorithms have the same starting points except Newton's algorithm, since it failed to converge for all of them. \relax }{table.caption.27}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Numerical verification of gradient and Hessian}{I}{appendix.A}}
\newlabel{app:statPoints}{{A}{I}{Numerical verification of gradient and Hessian}{appendix.A}{}}
\@writefile{toc}{\contentsline {paragraph}{Stationary point \#1:}{I}{section*.28}}
\@writefile{toc}{\contentsline {paragraph}{Stationary point \#2:}{I}{section*.29}}
\@writefile{toc}{\contentsline {paragraph}{Stationary point \#3:}{I}{section*.30}}
\@writefile{toc}{\contentsline {paragraph}{Stationary point \#4:}{I}{section*.31}}
\@writefile{toc}{\contentsline {paragraph}{Stationary point \#5:}{I}{section*.32}}
\@writefile{toc}{\contentsline {paragraph}{Stationary point \#6:}{II}{section*.33}}
\@writefile{toc}{\contentsline {paragraph}{Stationary point \#7:}{II}{section*.34}}
\@writefile{toc}{\contentsline {paragraph}{Stationary point \#8:}{II}{section*.35}}
\@writefile{toc}{\contentsline {paragraph}{Stationary point \#9:}{II}{section*.36}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Code for Part 1}{III}{appendix.B}}
\newlabel{code:part1}{{B}{III}{Code for Part 1}{appendix.B}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1}OptimalN.m}{III}{subsection.B.1}}
\@writefile{lol}{\contentsline {lstlisting}{../Part1/OptimalN.m}{III}{lstlisting.-1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2}NOfit.m}{V}{subsection.B.2}}
\@writefile{lol}{\contentsline {lstlisting}{../Part1/NOfit.m}{V}{lstlisting.-2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.3}autocorrelationTest.m}{VI}{subsection.B.3}}
\@writefile{lol}{\contentsline {lstlisting}{../Part1/autocorrelationTest.m}{VI}{lstlisting.-3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.4}runTest.m}{VII}{subsection.B.4}}
\@writefile{lol}{\contentsline {lstlisting}{../Part1/runTest.m}{VII}{lstlisting.-4}}
\@writefile{toc}{\contentsline {section}{\numberline {C}Code for Part 2}{VIII}{appendix.C}}
\newlabel{code:part2}{{C}{VIII}{Code for Part 2}{appendix.C}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.1}himmelblau.m}{VIII}{subsection.C.1}}
\@writefile{lol}{\contentsline {lstlisting}{../Part2/himmelblau.m}{VIII}{lstlisting.-5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.2}steepest\_descent.m}{VIII}{subsection.C.2}}
\@writefile{lol}{\contentsline {lstlisting}{../Part2/steepest\textunderscore descent.m}{VIII}{lstlisting.-6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.3}newton.m}{IX}{subsection.C.3}}
\@writefile{lol}{\contentsline {lstlisting}{../Part2/newton.m}{IX}{lstlisting.-7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.4}quasi\_newton.m}{X}{subsection.C.4}}
\@writefile{lol}{\contentsline {lstlisting}{../Part2/quasi\textunderscore newton.m}{X}{lstlisting.-8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.5}Gauss\_Newton.m}{XII}{subsection.C.5}}
\@writefile{lol}{\contentsline {lstlisting}{../Part2/Gauss\textunderscore Newton.m}{XII}{lstlisting.-9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.6}Levenberg\_Marquardt.m}{XIII}{subsection.C.6}}
\@writefile{lol}{\contentsline {lstlisting}{../Part2/Levenberg\textunderscore Marquardt.m}{XIII}{lstlisting.-10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.7}matlab\_optimization.m}{XV}{subsection.C.7}}
\@writefile{lol}{\contentsline {lstlisting}{../Part2/matlab\textunderscore optimization.m}{XV}{lstlisting.-11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.8}evalStatPoints.m}{XV}{subsection.C.8}}
\newlabel{code:evalStatPoints}{{C.8}{XV}{evalStatPoints.m}{subsection.C.8}{}}
\@writefile{lol}{\contentsline {lstlisting}{../Part2/evalStatPoints.m}{XV}{lstlisting.-12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.9}backtracking.m}{XVI}{subsection.C.9}}
\@writefile{lol}{\contentsline {lstlisting}{../Part2/backtracking.m}{XVI}{lstlisting.-13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.10}errorplot.m}{XVII}{subsection.C.10}}
\@writefile{lol}{\contentsline {lstlisting}{../Part2/errorplot.m}{XVII}{lstlisting.-14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.11}himmelblauContours.m}{XVII}{subsection.C.11}}
\@writefile{lol}{\contentsline {lstlisting}{../Part2/himmelblauContours.m}{XVII}{lstlisting.-15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.12}muplot.m}{XVIII}{subsection.C.12}}
\@writefile{lol}{\contentsline {lstlisting}{../Part2/muplot.m}{XVIII}{lstlisting.-16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.13}saveTable.m}{XVIII}{subsection.C.13}}
\@writefile{lol}{\contentsline {lstlisting}{../Part2/saveTable.m}{XVIII}{lstlisting.-17}}
\zref@newlabel{LastPage}{\default{C.13}\page{XVIII}\abspage{40}}
